<!-- Main -->

<!-- One -->
<section id="one">
	<div class="inner">
		<header class="major">
			<h1>Spark in Data Engineering</h2>
			</header>
            <!--
			<div class="content">
				<p style="font-size: 30px; ">S3&nbsp;&nbsp;&nbsp;&nbsp;|
					&nbsp;&nbsp;&nbsp;&nbsp;GLUE&nbsp;&nbsp;&nbsp;&nbsp;|
					&nbsp;&nbsp;&nbsp;&nbsp;LAMBDA&nbsp;&nbsp;&nbsp;&nbsp;|
					&nbsp;&nbsp;&nbsp;&nbsp;EMR&nbsp;&nbsp;&nbsp;&nbsp;|
					&nbsp;&nbsp;&nbsp;&nbsp;REDSHIFT&nbsp;&nbsp;&nbsp;&nbsp;|
					&nbsp;&nbsp;&nbsp;&nbsp;ATHENA&nbsp;&nbsp;|
				&nbsp;&nbsp;&nbsp;&nbsp;CLOUDWATCH</p> 
			</div>
        -->
    </div>
</section>

<!-- Two -->
<section id="two" class="spotlights">
  <section>
  </a>
  <div class="content">
    <div class="inner">
       <header class="major">
          <h3> 1. Data Ingestion</h3>
      </header>
      <p>Ingested large volumes of structured and semi-structured data from sources like Kafka, S3, and relational databases using Spark.<br>

Built batch and near real-time ingestion pipelines to handle high-velocity transactional and event-driven data.</p>

  </div>
</div>
</section>
<section>
 <div class="content">
    <div class="inner">
       <header class="major">
          <h3>2. Data Transformation & Enrichment</h3>
      </header>
      <p>Implemented real-time data pipelines using Spark Structured Streaming to process API events and Kafka topics.<br>

Applied window functions, watermarking, and incremental data processing to support near real-time analytics.</p>
  </div>
</div>
</section>
<section>
 <div class="content">
    <div class="inner">
       <header class="major">
          <h3>3. Real-Time Streaming</h3>
      </header>
      <p>Wrote Python scripts for data validation, anomaly detection and schema enforcement.<br>

      Automated row count verification, schema matching and threshold-based anomaly detection using custom Python logic.</p>
  </div>
</div>
</section>

<section>
 <div class="content">
    <div class="inner">
       <header class="major">
          <h3>4. Performance Optimization</h3>
      </header>
      <p>Tune Spark configurations for executor memory, partitioning strategies, and shuffle management to improve job efficiency.

Use Parquet file format, predicate pushdown, and caching techniques to reduce I/O and processing time.</p>
  </div>
</div>
</section>

<section>
 <div class="content">
    <div class="inner">
       <header class="major">
          <h3>5. Data Pipeline Automation</h3>
      </header>
      <p>Integrated Spark jobs into Airflow DAGs using SparkSubmitOperator for seamless orchestration and monitoring.<br>

Automated pipeline dependencies, retries, and failure handling to ensure robust data processing workflows.</p>
  </div>
</div>
</section>
<section>
 <div class="content">
    <div class="inner">
       <header class="major">
          <h3>6. Data Loading and Integration</h3>
      </header>
      <p>Used Spark to write final datasets to Amazon Redshift, S3, and Cassandra for downstream consumption.<br>

Leveraged Spark-SQL to prepare staging files optimized for Redshift COPY and bulk loads.</p>
  </div>
</div>
</section>

</section>